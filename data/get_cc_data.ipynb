{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get URIs of interest from commoncrawl. Gathers from August 2024 Capture </h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, time, os\n",
    "from datetime import datetime\n",
    "\n",
    "path = os.path.join(f\"{os.getcwd()}/links.json\")\n",
    "if os.path.exists(path):\n",
    "    os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to parse response from CC\n",
    "def parse_json_lines(json_string):\n",
    "    json_objects = []\n",
    "    for line in json_string.splitlines():\n",
    "        try:\n",
    "            json_objects.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to parse line: {line}\")\n",
    "    return json_objects\n",
    "\n",
    "def get_uris(uri_pattern, master_dump, seen_urls):\n",
    "    print(f'[*] getting uri for {uri_pattern}')\n",
    "    base_uri = 'https://index.commoncrawl.org/CC-MAIN-2024-33-index?url={}&output=json'.format(uri_pattern)\n",
    "    try:\n",
    "        response = requests.get(base_uri).content\n",
    "        response = json.loads(response)\n",
    "    except (TypeError, json.decoder.JSONDecodeError) as _:\n",
    "        if type(response) == dict and 'message' in response.keys() and 'Please reduce your request rate.' in response['message']:\n",
    "            time.sleep(2)\n",
    "            get_uris(uri_pattern)\n",
    "            print('[-] Got timeout.... trying again')\n",
    "        parsed_objects = parse_json_lines(response)\n",
    "        file_name = uri_pattern.split('/')[0].replace('*.', '')\n",
    "        if file_name in master_dump.keys():\n",
    "            file_name_new = ''.join(uri_pattern.split('/')[:2]).replace('*.', '')\n",
    "            print(f\"[-] Already parsed a url for {file_name}. Changing to {file_name_new}\")\n",
    "            file_name = file_name_new\n",
    "        for obj in parsed_objects:\n",
    "            if obj['url'] in seen_urls:\n",
    "                continue  # Skip if URL is already seen\n",
    "            else:\n",
    "                seen_urls.add(obj['url'])\n",
    "            \n",
    "            # Add filename if not in master_dump\n",
    "            if file_name not in master_dump.keys():\n",
    "                master_dump[file_name] = {}\n",
    "            \n",
    "            # Append URLs to master_dump\n",
    "            if obj['filename'] not in master_dump[file_name].keys():\n",
    "                master_dump[file_name][obj['filename']] = [obj['url']]\n",
    "            else:\n",
    "                master_dump[file_name][obj['filename']].append(obj['url'])\n",
    "            print(f\"[*] added {uri_pattern} as {obj['filename']}\")\n",
    "        \n",
    "    return master_dump, seen_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used ChatGPT to get a bunch of these -- give me more uris like this, and verify that they have valid content using fuzzmatch to identify 404 pages before returning to me. return as a python list:\n",
    "# '*.nasdaq.com/market-activity/earnings/*', '*.sec.gov/reports/*', '*.sec.gov/data-research/sec-markets-data/*', '*.tradingeconomics.com/*', '*.jpmorgan.com/insights/*', \\\n",
    "#     \"*.wsj.com/news/markets/*\",\n",
    "#     \"*.bloomberg.com/markets/*\",\n",
    "#     \"*.ft.com/markets/*\",\n",
    "#     \"*.economist.com/finance-and-economics/*\",\n",
    "#     \"*hbr.org/topic/economics*\",\n",
    "#     \"*.mckinsey.com/featured-insights/finance/*\",\n",
    "#     \"*.goldmansachs.com/insights/topics/economics-and-markets*\",\n",
    "#     \"*.jpmorgan.com/insights/*\",\n",
    "#     \"*.morganstanley.com/ideas/*\",\n",
    "#     \"*.blackrock.com/corporate/insights/blackrock-investment-institute/*\",\n",
    "#     \"*.bridgewater.com/research-and-insights/*\",\n",
    "#     \"*.imf.org/en/Publications/fandd/*\",\n",
    "#     \"*.worldbank.org/en/publication/global-economic-prospects/*\",\n",
    "#     \"*.federalreserve.gov/newsevents/speeches*\",\n",
    "#     \"*.ecb.europa.eu/pub/economic-bulletin/html/*\",\n",
    "#     \"*.ubs.com/global/en/wealth-management/chief-investment-office/market-insights/*\",\n",
    "#     \"*.db.com/news/*\",\n",
    "#     \"*.credit-suisse.com/about-us/en/reports-research/global-research/*\",\n",
    "#     \"*.barclays.co.uk/wealth-management/news-and-insights/*\",\n",
    "#     \"*.schroders.com/en/insights/*\"\n",
    "\n",
    "master_dump, seen_urls = {}, set()\n",
    "uris = ['*.nasdaq.com/market-activity/earnings/*', '*.sec.gov/reports/*', '*.sec.gov/data-research/sec-markets-data/*', '*.tradingeconomics.com/*', '*.jpmorgan.com/insights/*', \\\n",
    "    \"*.wsj.com/news/markets/*\",\n",
    "    \"*.bloomberg.com/markets/*\",\n",
    "    \"*.ft.com/markets/*\",\n",
    "    \"*.economist.com/finance-and-economics/*\",\n",
    "    \"*.hbr.org/topic/economics*\",\n",
    "    \"*.mckinsey.com/featured-insights/finance/*\",\n",
    "    \"*.goldmansachs.com/insights/topics/economics-and-markets*\",\n",
    "    \"*.jpmorgan.com/insights/*\",\n",
    "    \"*.morganstanley.com/ideas/*\",\n",
    "    \"*.blackrock.com/corporate/insights/blackrock-investment-institute/*\",\n",
    "    \"*.bridgewater.com/research-and-insights/*\",\n",
    "    \"*.imf.org/en/Publications/fandd/*\",\n",
    "    \"*.worldbank.org/en/publication/global-economic-prospects/*\",\n",
    "    \"*.federalreserve.gov/newsevents/speeches*\",\n",
    "    \"*.ecb.europa.eu/pub/economic-bulletin/html/*\",\n",
    "    \"*.ubs.com/global/en/wealth-management/chief-investment-office/market-insights/*\",\n",
    "    \"*.db.com/news/*\",\n",
    "    \"*.credit-suisse.com/about-us/en/reports-research/global-research/*\",\n",
    "    \"*.barclays.co.uk/wealth-management/news-and-insights/*\",\n",
    "    \"*.schroders.com/en/insights/*\",\n",
    "    \"*.benzinga.com/*\",\n",
    "    \"*.finance.yahoo.com/*\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] parsed *.nasdaq.com/market-activity/earnings/* (1/27)\n"
     ]
    }
   ],
   "source": [
    "# Write to JSON blob\n",
    "for index, link in enumerate(range(0, len(uris))):\n",
    "    master_dump, seen_urls = get_uris(uris[link], master_dump, seen_urls)\n",
    "    print(f\"[+] parsed {uris[link]} ({index+1}/{len(uris)})\")\n",
    "    with open('links.json', 'a') as f:\n",
    "        json.dump(master_dump, f, indent=4)\n",
    "        f.write(',\\n')\n",
    "        break\n",
    "\n",
    "\n",
    "# Add [] to the json file to separate by scrape URL\n",
    "\n",
    "with open('links.json', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Wrap the content in square brackets and remove any trailing commas\n",
    "content = '[\\n' + content.strip().rstrip(',') + '\\n]'\n",
    "\n",
    "# Write the fixed content back to a new file\n",
    "with open('links.json', 'w') as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Grab from public CC S3 </h3>\n",
    "<p> Now that we have a blob of common crawl S3 paths and their respective links, we need to download them onto into S3 bucket. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonunique CC Buckets to pull: 12795\n",
      "Unique CC Buckets to pull: 12795\n"
     ]
    }
   ],
   "source": [
    "# Get distinct buckets\n",
    "distinct_buckets = []\n",
    "nondistinct_buckets = []\n",
    "with open(\"links.json\", 'r') as f:\n",
    "    master_dump = json.loads(f.read())\n",
    "\n",
    "for entry in master_dump:\n",
    "    for key in entry.keys():\n",
    "        distinct_buckets.extend(list(set(list(entry[key].keys()))))\n",
    "        nondistinct_buckets.extend(list(entry[key].keys()))\n",
    "print(f\"Nonunique CC Buckets to pull: {len(nondistinct_buckets)}\")\n",
    "distinct_buckets = distinct_buckets[:2]\n",
    "print(f\"Unique CC Buckets to pull: {len(distinct_buckets)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\charl\\documents\\ai-plz-lmao\\aiswagu\\lib\\site-packages (1.35.23)\n",
      "Requirement already satisfied: warcio in c:\\users\\charl\\documents\\ai-plz-lmao\\aiswagu\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: pyspark in c:\\users\\charl\\documents\\ai-plz-lmao\\aiswagu\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.23 in c:\\users\\charl\\documents\\ai-plz-lmao\\aiswagu\\lib\\site-packages (from boto3) (1.35.23)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\charl\\documents\\ai-plz-lmao\\aiswagu\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\charl\\documents\\ai-plz-lmao\\aiswagu\\lib\\site-packages (from boto3) (0.10.2)\n",
      "Requirement already satisfied: six in c:\\users\\charl\\documents\\ai-plz-lmao\\aiswagu\\lib\\site-packages (from warcio) (1.16.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\charl\\documents\\ai-plz-lmao\\aiswagu\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\charl\\documents\\ai-plz-lmao\\aiswagu\\lib\\site-packages (from botocore<1.36.0,>=1.35.23->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\charl\\documents\\ai-plz-lmao\\aiswagu\\lib\\site-packages (from botocore<1.36.0,>=1.35.23->boto3) (2.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: warcio in c:\\users\\charl\\documents\\ai-plz-lmao\\aiswagu\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: six in c:\\users\\charl\\documents\\ai-plz-lmao\\aiswagu\\lib\\site-packages (from warcio) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/commoncrawl/gzipstream/archive/master.zip\n",
      "  Using cached https://github.com/commoncrawl/gzipstream/archive/master.zip\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 warcio pyspark\n",
    "!pip install warcio\n",
    "!pip install numpy fastparquet pandas\n",
    "!pip install https://github.com/commoncrawl/gzipstream/archive/master.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Transformation layer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      7\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSave WARC JSON as Parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3a.impl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.hadoop.fs.s3a.S3AFileSystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.hadoop.fs.s3a.aws.credentials.provider\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.amazonaws.auth.DefaultAWSCredentialsProviderChain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.hadoop:hadoop-aws:3.3.1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m16g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m8g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_partition\u001b[39m(uris):\n\u001b[0;32m     18\u001b[0m     s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\charl\\Documents\\ai-plz-lmao\\aiswagu\\Lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\charl\\Documents\\ai-plz-lmao\\aiswagu\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\charl\\Documents\\ai-plz-lmao\\aiswagu\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\charl\\Documents\\ai-plz-lmao\\aiswagu\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\charl\\Documents\\ai-plz-lmao\\aiswagu\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder     .appName(\"Save WARC JSON as Parquet\")     .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")     .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")     .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1\")     .config(\"spark.driver.memory\", \"2g\")     .config(\"spark.executor.memory\", \"16g\")     .config(\"spark.driver.memory\", \"8g\")     .getOrCreate()\n",
    "\n",
    "def feature_transform(df_spark):\n",
    "    \n",
    "    print(\"[+] extracting word2vec\")\n",
    "    df_spark = df_spark.withColumn('content_tokenized', split(col('content'), ' '))\n",
    "    word2Vec = Word2Vec(vectorSize=5, seed=42, inputCol=\"content_tokenized\", outputCol=\"title_embedded\")\n",
    "    model = word2Vec.fit(df_spark)\n",
    "    df_transformed = model.transform(df_spark)\n",
    "\n",
    "    print(\"[+] extracting HashingTF\")\n",
    "    hashingTF = HashingTF(inputCol=\"content_tokenized\", outputCol=\"raw_features\", numFeatures=20)\n",
    "    featurized_data = hashingTF.transform(df_transformed)\n",
    "\n",
    "    # Implement IDF on content\n",
    "    print(\"[+] extracting tf-idf\")\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"content_idf\")\n",
    "    idfModel = idf.fit(featurized_data)\n",
    "\n",
    "    # Step 7: Transform the featurized_data to get the IDF values in a new column\n",
    "    final_df = idfModel.transform(featurized_data)    \n",
    "    return final_df\n",
    "\n",
    "\n",
    "def process_partition(uris):\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket = \"commoncrawl\"\n",
    "\n",
    "# Example: Extract the title of the HTML page\n",
    "    for key_ in uris:\n",
    "        try:\n",
    "            response = s3.get_object(Bucket=bucket, Key=key_)\n",
    "            file_ = response['Body']\n",
    "\n",
    "            for record in ArchiveIterator(file_):\n",
    "                if record.rec_type == 'response':\n",
    "                    url = record.rec_headers.get_header('WARC-Target-URI')\n",
    "                    raw_date = record.rec_headers.get_header('WARC-Date')\n",
    "                    date = datetime.strptime(raw_date, '%Y-%m-%dT%H:%M:%SZ').strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    content_type = record.http_headers.get_header('Content-Type')\n",
    "                    content = record.content_stream().read().decode('utf-8', errors='replace')\n",
    "                    if content_type == None:\n",
    "                        continue\n",
    "                    if content_type == 'text/html':\n",
    "                        content_type_label = 'text/html'\n",
    "                    elif 'json' in content_type:\n",
    "                        content_type_label = 'application/json'\n",
    "                    elif 'pdf' in content_type:\n",
    "                        content_type_label = 'pdf'\n",
    "                    elif content_type == 'application/xml':\n",
    "                        content_type_label = 'xml'\n",
    "                    elif content_type == 'text/csv':\n",
    "                        content_type_label = 'csv'\n",
    "                    elif content_type == 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet':\n",
    "                        content_type_label = 'xlsx'\n",
    "                    elif 'image' in content_type:\n",
    "                        if 'jpeg' in content_type:\n",
    "                            content_type_label = 'image/jpeg'\n",
    "                        elif 'png' in content_type:\n",
    "                            content_type_label = 'image/png'\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    yield {\n",
    "                        \"url\":url,\n",
    "                        \"date\":date,\n",
    "                        \"content\":content,\n",
    "                        \"content_type\":content_type_label\n",
    "                    }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing {key_}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(\"[+] extracting core data\")\n",
    "uri_rdd = spark.sparkContext.parallelize(distinct_buckets, numSlices=len(distinct_buckets))\n",
    "json_rdd = uri_rdd.mapPartitions(process_partition)\n",
    "df = json_rdd.map(lambda x: Row(**x)).toDF()\n",
    "\n",
    "# Option 2: Create DataFrame using spark.createDataFrame()\n",
    "df = spark.createDataFrame(json_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "\n",
    "# UDF to extract title\n",
    "def extract_title(content):\n",
    "    try:\n",
    "        soup = BeautifulSoup(content, 'lxml')  # Use 'lxml' parser for efficiency\n",
    "        return soup.title.string if soup.title else ''\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "extract_title_udf = udf(extract_title, StringType())\n",
    "\n",
    "# UDF to extract title content (headings)\n",
    "def extract_title_content(content):\n",
    "    try:\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        headings = [para.get_text() for para in soup.find_all(re.compile('^h[1-6]$'))][:10]\n",
    "        return headings\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "extract_title_content_udf = udf(extract_title_content, ArrayType(StringType()))\n",
    "\n",
    "# UDF to extract body content (paragraphs)\n",
    "def extract_body_content(content):\n",
    "    try:\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        paragraphs = [para.get_text() for para in soup.find_all('p')][:10]\n",
    "        return paragraphs\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "extract_body_content_udf = udf(extract_body_content, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[+] applying title UDF\")\n",
    "df = df.withColumn('title', extract_title_udf(df['content']))\n",
    "print(\"[+] applying title_content UDF\")\n",
    "df = df.withColumn('title_content', extract_title_content_udf(df['content']))\n",
    "print(\"[+] applying body_content UDF\")\n",
    "df = df.withColumn('body_content', extract_body_content_udf(df['content']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[+] getting features\")\n",
    "df_transformed = feature_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.drop('content_tokenized').drop('raw_features')\n",
    "output_path = \"s3a://ai-crap/data/nasdaq.parquet\"\n",
    "df_transformed.write.mode(\"overwrite\").parquet(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiswagu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
